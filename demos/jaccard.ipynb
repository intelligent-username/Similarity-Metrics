{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ca9171",
   "metadata": {},
   "source": [
    "# Different Usecases for Jaccard Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4dd81c",
   "metadata": {},
   "source": [
    "## A simple implementation of autocorrect\n",
    "\n",
    "The [original dataset](https://www.kaggle.com/datasets/anthonytherrien/dictionary-of-english-words-and-definitions) was cleaned up as we only need the words in this case.\n",
    "\n",
    "Note: in later implementations, we can add more details for the autocorrect suggestions, such as letter ordering, word length, 'letter distance', and even sentence context. Right now we only filter out by word length.\n",
    "\n",
    "Note that this is nowhere near fast enough to actually use in phones, etc. it's just a naive demonstration of jaccard distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d01c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 suggestions:\n",
      "halo\n",
      "hell\n",
      "hello\n",
      "helm\n",
      "help\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return edit_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "def ngram_jaccard_distance(s1, s2, n=2):\n",
    "    def get_ngrams(text, n):\n",
    "        return set([text[i:i+n] for i in range(len(text)-n+1)])\n",
    "    \n",
    "    set1 = get_ngrams(s1, n)\n",
    "    set2 = get_ngrams(s2, n)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return 1 - intersection / union if union > 0 else 0\n",
    "\n",
    "def suggestion(word):\n",
    "    with open('../data/dictionary.txt', 'r', encoding='utf-16') as f:\n",
    "        input_word = word\n",
    "        dictionary = f.read().splitlines()\n",
    "        candidates = []\n",
    "        for candidate in dictionary:\n",
    "            if abs(len(candidate) - len(input_word)) <= 1:\n",
    "                distance = edit_distance(input_word, candidate)\n",
    "                candidates.append((candidate, distance))\n",
    "\n",
    "        # Sort by edit distance\n",
    "        candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "        if candidates and candidates[0][1] == 0 and candidates[0][0] == input_word:\n",
    "            print(f\"The word {input_word} is already in dictionary, but the next top 5 suggestions are:\")\n",
    "            top_suggestions = candidates[1:6]\n",
    "\n",
    "        else:\n",
    "            print(\"Top 5 suggestions:\")\n",
    "            top_suggestions = candidates[:5]\n",
    "\n",
    "        for candidate, _ in top_suggestions:\n",
    "            print(candidate)\n",
    "\n",
    "    # If this was a real function we would return words, of course. \n",
    "\n",
    "suggestion('helo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d0798",
   "metadata": {},
   "source": [
    "## Identifying if two people are family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ba129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting demo run...\n",
      "[INFO] Searching NCBI for query: 'BRCA1[Gene] AND Homo sapiens[Organism]' (max 10)...\n",
      "[INFO] Found 10 sequence IDs.\n",
      "[INFO] Fetching 10 sequences from NCBI in bulk...\n",
      "[INFO] Successfully retrieved 10 sequences.\n",
      "[INFO] Beginning similarity analysis...\n",
      "[INFO] Generating 10-mers for Person 1...\n",
      "[INFO] Comparing Person 1 against 9 other sequences...\n",
      "   -> Processing Person 2...\n",
      "   -> Processing Person 3...\n",
      "   -> Processing Person 4...\n",
      "   -> Processing Person 5...\n",
      "   -> Processing Person 6...\n",
      "   -> Processing Person 7...\n",
      "   -> Processing Person 8...\n",
      "   -> Processing Person 9...\n",
      "   -> Processing Person 10...\n",
      "[INFO] Similarity results (sorted by distance):\n",
      "Person 1: Jaccard distance = 0.035\n",
      "  -> Might be related\n",
      "Person 2: Jaccard distance = 0.998\n",
      "Person 9: Jaccard distance = 1.000\n",
      "Person 5: Jaccard distance = 1.000\n",
      "Person 8: Jaccard distance = 1.000\n",
      "Person 4: Jaccard distance = 1.000\n",
      "Person 3: Jaccard distance = 1.000\n",
      "Person 6: Jaccard distance = 1.000\n",
      "Person 7: Jaccard distance = 1.000\n",
      "[INFO] Script finished.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from Bio import Entrez, SeqIO\n",
    "from io import StringIO\n",
    "\n",
    "sys.path.append('..')\n",
    "from distances.jac import jaccard_distance\n",
    "\n",
    "Entrez.email = \"your_real_email@domain.com\"  # must be valid per NCBI policy\n",
    "\n",
    "def fetch_sequences(query, max_samples=5):\n",
    "    \"\"\"\n",
    "    Fetch multiple sequences at once using Entrez, more efficient than one-by-one efetch.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Searching NCBI for query: '{query}' (max {max_samples})...\")\n",
    "    search_handle = Entrez.esearch(db=\"nucleotide\", term=query, retmax=max_samples)\n",
    "    record = Entrez.read(search_handle)\n",
    "    ids = record['IdList']\n",
    "    print(f\"[INFO] Found {len(ids)} sequence IDs.\")\n",
    "\n",
    "    if not ids:\n",
    "        print(\"[WARN] No sequences found for the query.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"[INFO] Fetching {len(ids)} sequences from NCBI in bulk...\")\n",
    "    fetch_handle = Entrez.efetch(db=\"nucleotide\", id=\",\".join(ids), rettype=\"fasta\", retmode=\"text\")\n",
    "    seq_records = list(SeqIO.parse(StringIO(fetch_handle.read()), \"fasta\"))\n",
    "    print(f\"[INFO] Successfully retrieved {len(seq_records)} sequences.\")\n",
    "    return [str(rec.seq) for rec in seq_records]\n",
    "\n",
    "def find_similarity(person1, people, k=10, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Compute similarity between one sequence and a list of others using k-mer Jaccard.\n",
    "    Uses multiset counts (bag-of-kmers) for more robustness.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    def get_kmers(seq, k):\n",
    "        return Counter([seq[i:i+k] for i in range(len(seq)-k+1)])\n",
    "\n",
    "    def jaccard_multiset(c1, c2):\n",
    "        # intersection over union using min/max counts\n",
    "        intersection = sum((c1 & c2).values())\n",
    "        union = sum((c1 | c2).values())\n",
    "        return 1 - intersection / union if union else 1.0\n",
    "\n",
    "    print(f\"[INFO] Generating {k}-mers for Person 1...\")\n",
    "    set1 = get_kmers(person1, k)\n",
    "\n",
    "    print(f\"[INFO] Comparing Person 1 against {len(people)} other sequences...\")\n",
    "    similarities = []\n",
    "    for i, person in enumerate(people):\n",
    "        print(f\"   -> Processing Person {i+2}...\")\n",
    "        set2 = get_kmers(person, k)\n",
    "        dist = jaccard_multiset(set1, set2)\n",
    "        similarities.append((i, dist))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1])\n",
    "    \n",
    "    print(\"[INFO] Similarity results (sorted by distance):\")\n",
    "    for idx, dist in similarities:\n",
    "        print(f\"Person {idx+1}: Jaccard distance = {dist:.3f}\")\n",
    "        if dist < cutoff:\n",
    "            print(\"  -> Might be related\")\n",
    "\n",
    "# demo\n",
    "print(\"Starting run...\")\n",
    "print(\"This will take a while, especially because it's in Python\")\n",
    "samples = fetch_sequences(\"BRCA1[Gene] AND Homo sapiens[Organism]\", max_samples=10)\n",
    "if samples:\n",
    "    person1 = samples[0]\n",
    "    people = samples[1:]\n",
    "    print(\"[INFO] Beginning similarity analysis...\")\n",
    "    find_similarity(person1, people)\n",
    "print(\"Done running.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
